## Explainable AI papers Year-wise
List of papers in the area of Explainable Artificial Intelligence Year wise


**2014** 


Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps [pdf](https://arxiv.org/pdf/1312.6034.pdf)

Visualizing and understanding convolutional networks [pdf](https://arxiv.org/pdf/1311.2901.pdf)

Object detectors emerge in deep scene CNN’s [pdf](https://arxiv.org/pdf/1412.6856.pdf)


**2015**

Understanding Neural Network through Deep Visualization [pdf](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)

On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation [pdf](https://pdfs.semanticscholar.org/17a2/73bbd4448083b01b5a9389b3c37f5425aac0.pdf?_ga=2.53370664.129866596.1560509495-1781895300.1541438724)

Understanding Deep Image Representation by Inverting Them [pdf](https://arxiv.org/pdf/1412.0035.pdf)

Striving for Simplicity: The All Convolutional Net [pdf](https://arxiv.org/pdf/1412.6806.pdf)


**2016**


An unexpected unity among methods for interpreting model predictions [pdf](https://arxiv.org/pdf/1611.07478.pdf)

“Why Should I Trust you?” Explaining the Predictions of Any Classifier [pdf](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)

Learning deep features for discriminative localization [pdf](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)


**2017**

SmoothGrad: Removing Noise By Adding Noise [pdf](https://arxiv.org/pdf/1706.03825.pdf)

Axiomatic Attribution for Deep Neural Networks [pdf](https://arxiv.org/pdf/1703.01365.pdf)

Learning Important Features Through Propagating Activation Differences [pdf](https://arxiv.org/pdf/1704.02685.pdf)

Understanding Black-box Predictions via Influence Functions [pdf](http://proceedings.mlr.press/v70/koh17a/koh17a.pdf)

Interpretable Explanations of Black Boxes by Meaningful Perturbation [pdf](http://www.robots.ox.ac.uk/~vedaldi//assets/pubs/fong17interpretable.pdf)

Visualizing deep neural network decisions: Prediction difference analysis [pdf](https://arxiv.org/pdf/1702.04595.pdf)

Network Dissection: Quantifying Interpretability of Deep Visual Representations [pdf](https://arxiv.org/pdf/1704.05796.pdf)

The (Un)reliability of Saliency Methods [pdf](https://arxiv.org/pdf/1711.00867.pdf)

Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations [pdf](https://arxiv.org/pdf/1703.03717.pdf)

Understanding intermediate layers using linear classifier probes [pdf](https://arxiv.org/pdf/1610.01644.pdf)

Using KL-divergence to focus Deep Visual Explanation [pdf](https://arxiv.org/pdf/1711.06431.pdf)

A Unified Approach to Interpreting Model Predictions [pdf](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf)

Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization [pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf)


**2018**

Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions [pdf](https://arxiv.org/pdf/1710.04806.pdf)

Towards Robust Interpretability with Self-Explaining Neural Networks [pdf](https://papers.nips.cc/paper/8003-towards-robust-interpretability-with-self-explaining-neural-networks.pdf)

Towards Better Understanding of Gradient-Based Attribution Methods For Deep Neural Networks [pdf](https://arxiv.org/pdf/1711.06104.pdf)

Interpretable Convolutional Neural Networks [pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.pdf)

Grad-cam++: Improved Visual Explanations for Deep Convolutional Networks [pdf](https://arxiv.org/pdf/1711.06104.pdf)

Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in deep Neural Networks [pdf](https://arxiv.org/pdf/1710.11063.pdf)

Sanity Checks for Saliency Maps [pdf](https://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf)

Imagenet trained CNN’s are biased towards texture, increasing shape bias improves accuracy and robustness [pdf](https://arxiv.org/pdf/1811.12231.pdf)

Anchors: High precision model agnostic explanations [pdf](https://homes.cs.washington.edu/~marcotcr/aaai18.pdf)


**2019**

A Simple Saliency Method That Passes the Sanity Check  
Bias Also Matters: Bias Attribution for Deep Neural Network Explanation
Counterfactual Visual Explanations
Explainable AI for Trees: From Local Explanations to Global Understanding
G. Pacaci, D. Johnson, S. McKeever, A. Hamfelt, “ Why did you do that?: Explaining black-box models with inductive synthesis “
M.Hind, D.Wei, M. Campbell, N.C. Codella, A. Dhurandhar, A. Mojsilovic, K.N. Ramamurthy, K.R. Varshney, “TED: Teaching AI to explain its decisions “
This Looks Like That: Deep Learning for Interpretable Image Recognition 
Interpretable Image Recognition with Hierarchical Prototypes
Understanding Deep Neural Networks For Regression In Leaf Counting
Score-CAM: Improved Visual Explanations Via Score-Weighted Class Activation Mapping 
 Black Box Explanation by Learning Image Exemplars in the Latent Feature Space
Learning Reliable Visual Saliency for Model Explanations 


Workshops
https://xai.kdd2019.a.intuit.com/
https://human-centered.ai/methods-of-explainable-ai/
https://sites.google.com/view/xai2019/home
http://www.heatmapping.org/ ( contains a list of workshops )



