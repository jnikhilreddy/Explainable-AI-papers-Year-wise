## Explainable AI papers Year-wise
List of papers in the area of Explainable Artificial Intelligence Year wise


**2014** 


Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps [pdf](https://arxiv.org/pdf/1312.6034.pdf)

Visualizing and understanding convolutional networks [pdf](https://arxiv.org/pdf/1311.2901.pdf)

Object detectors emerge in deep scene CNN’s [pdf](https://arxiv.org/pdf/1412.6856.pdf)

**2015**
Understanding Neural Network through Deep Visualization [pdf](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)

On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation [pdf](https://pdfs.semanticscholar.org/17a2/73bbd4448083b01b5a9389b3c37f5425aac0.pdf?_ga=2.53370664.129866596.1560509495-1781895300.1541438724)


Understanding Deep Image Representation by Inverting Them [pdf](https://arxiv.org/pdf/1412.0035.pdf)

Striving for Simplicity: The All Convolutional Net [pdf](https://arxiv.org/pdf/1412.6806.pdf)

**2016**

An unexpected unity among methods for interpreting model predictions [pdf](https://arxiv.org/pdf/1611.07478.pdf)

“Why Should I Trust you?” Explaining the Predictions of Any Classifier [pdf](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)

Learning deep features for discriminative localization [pdf](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)


**2017**

SmoothGrad: Removing Noise By Adding Noise
Axiomatic Attribution for Deep Neural Networks
Learning Important Features Through Propagating Activation Differences
Understanding Black-box Predictions via Influence Functions
Interpretable Explanations of Black Boxes by Meaningful Perturbation
Visualizing deep neural network decisions: Prediction difference analysis
Network Dissection: Quantifying Interpretability of Deep Visual Representations
The (Un)reliability of Saliency Methods
Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations
Understanding intermediate layers using linear classifier probes
Using KL-divergence to focus Deep Visual Explanation
A Unified Approach to Interpreting Model Predictions
Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization

2018

Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions
Towards Robust Interpretability with Self-Explaining Neural Networks
Towards Better Understanding of Gradient-Based Attribution Methods For Deep Neural 
Networks
Interpretable Convolutional Neural Networks
Grad-cam++: Improved Visual Explanations for Deep Convolutional Networks
Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in deep Neural Networks
Sanity Checks for Saliency Maps
R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F.A. Wichmann, W. Brendel, “ Imagenet trained CNN’s are biased towards texture, increasing shape bias improves accuracy and robustness “
M.T. Riberio, S. Singh, C. Guestrin, “Anchors: High precision model agnostic explanations “


2019

A Simple Saliency Method That Passes the Sanity Check  
Bias Also Matters: Bias Attribution for Deep Neural Network Explanation
Counterfactual Visual Explanations
Explainable AI for Trees: From Local Explanations to Global Understanding
G. Pacaci, D. Johnson, S. McKeever, A. Hamfelt, “ Why did you do that?: Explaining black-box models with inductive synthesis “
M.Hind, D.Wei, M. Campbell, N.C. Codella, A. Dhurandhar, A. Mojsilovic, K.N. Ramamurthy, K.R. Varshney, “TED: Teaching AI to explain its decisions “
This Looks Like That: Deep Learning for Interpretable Image Recognition 
Interpretable Image Recognition with Hierarchical Prototypes
Understanding Deep Neural Networks For Regression In Leaf Counting
Score-CAM: Improved Visual Explanations Via Score-Weighted Class Activation Mapping 
 Black Box Explanation by Learning Image Exemplars in the Latent Feature Space
Learning Reliable Visual Saliency for Model Explanations 


Workshops
https://xai.kdd2019.a.intuit.com/
https://human-centered.ai/methods-of-explainable-ai/
https://sites.google.com/view/xai2019/home
http://www.heatmapping.org/ ( contains a list of workshops )



