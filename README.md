## Explainable AI papers Year-wise
List of papers in the area of Explainable Artificial Intelligence Year wise


2014 


Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps [link](https://arxiv.org/pdf/1312.6034.pdf), code

Visualizing and understanding convolutional networks [link](https://arxiv.org/pdf/1311.2901.pdf)

[Object detectors emerge in deep scene CNN’s](https://arxiv.org/abs/1412.6856)

2015

Understanding Neural Network through Deep Visualization
On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation
Understanding Deep Image Representation by Inverting Them
Striving for Simplicity: The All Convolutional Net

2016
An unexpected unity among methods for interpreting model predictions
“Why Should I Trust you?” Explaining the Predictions of Any Classifier
B. Zhou, A. Khosla, A. Lapedriza, A. Olivia, A. Torralba, “Learning deep features for discriminative localization “

2017

SmoothGrad: Removing Noise By Adding Noise
Axiomatic Attribution for Deep Neural Networks
Learning Important Features Through Propagating Activation Differences
Understanding Black-box Predictions via Influence Functions
Interpretable Explanations of Black Boxes by Meaningful Perturbation
Visualizing deep neural network decisions: Prediction difference analysis
Network Dissection: Quantifying Interpretability of Deep Visual Representations
The (Un)reliability of Saliency Methods
Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations
Understanding intermediate layers using linear classifier probes
Using KL-divergence to focus Deep Visual Explanation
A Unified Approach to Interpreting Model Predictions
Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization

2018

Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions
Towards Robust Interpretability with Self-Explaining Neural Networks
Towards Better Understanding of Gradient-Based Attribution Methods For Deep Neural 
Networks
Interpretable Convolutional Neural Networks
Grad-cam++: Improved Visual Explanations for Deep Convolutional Networks
Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in deep Neural Networks
Sanity Checks for Saliency Maps
R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F.A. Wichmann, W. Brendel, “ Imagenet trained CNN’s are biased towards texture, increasing shape bias improves accuracy and robustness “
M.T. Riberio, S. Singh, C. Guestrin, “Anchors: High precision model agnostic explanations “


2019

A Simple Saliency Method That Passes the Sanity Check  
Bias Also Matters: Bias Attribution for Deep Neural Network Explanation
Counterfactual Visual Explanations
Explainable AI for Trees: From Local Explanations to Global Understanding
G. Pacaci, D. Johnson, S. McKeever, A. Hamfelt, “ Why did you do that?: Explaining black-box models with inductive synthesis “
M.Hind, D.Wei, M. Campbell, N.C. Codella, A. Dhurandhar, A. Mojsilovic, K.N. Ramamurthy, K.R. Varshney, “TED: Teaching AI to explain its decisions “
This Looks Like That: Deep Learning for Interpretable Image Recognition 
Interpretable Image Recognition with Hierarchical Prototypes
Understanding Deep Neural Networks For Regression In Leaf Counting
Score-CAM: Improved Visual Explanations Via Score-Weighted Class Activation Mapping 
 Black Box Explanation by Learning Image Exemplars in the Latent Feature Space
Learning Reliable Visual Saliency for Model Explanations 


Workshops
https://xai.kdd2019.a.intuit.com/
https://human-centered.ai/methods-of-explainable-ai/
https://sites.google.com/view/xai2019/home
http://www.heatmapping.org/ ( contains a list of workshops )



